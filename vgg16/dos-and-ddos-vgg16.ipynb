{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9608010,"sourceType":"datasetVersion","datasetId":5862350}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-12T14:57:30.949610Z","iopub.execute_input":"2024-10-12T14:57:30.950024Z","iopub.status.idle":"2024-10-12T14:57:35.030392Z","shell.execute_reply.started":"2024-10-12T14:57:30.949967Z","shell.execute_reply":"2024-10-12T14:57:35.029415Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Check if CUDA is available and print the status\nif torch.cuda.is_available():\n    print(\"CUDA is available, using GPU\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"CUDA not available, using CPU\")\n    device = torch.device(\"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T14:57:35.032181Z","iopub.execute_input":"2024-10-12T14:57:35.033117Z","iopub.status.idle":"2024-10-12T14:57:35.070823Z","shell.execute_reply.started":"2024-10-12T14:57:35.033069Z","shell.execute_reply":"2024-10-12T14:57:35.069771Z"}},"outputs":[{"name":"stdout","text":"CUDA is available, using GPU\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Transform for data preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Dataset and Dataloaders\ntrain_dataset = datasets.ImageFolder(root='/kaggle/input/ddos-attack/attackImagesPaper/Train', transform=transform)\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/ddos-attack/attackImagesPaper/Test', transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nprint(f'Number of classes: {len(train_dataset.classes)}')\nprint(f'Class to index mapping: {train_dataset.class_to_idx}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T14:57:35.072004Z","iopub.execute_input":"2024-10-12T14:57:35.072313Z","iopub.status.idle":"2024-10-12T15:01:33.000562Z","shell.execute_reply.started":"2024-10-12T14:57:35.072281Z","shell.execute_reply":"2024-10-12T15:01:32.999539Z"}},"outputs":[{"name":"stdout","text":"Number of classes: 12\nClass to index mapping: {'10_Syn.csvImgs': 0, '11_TFTP.csvImgs_12k': 1, '12_UDPLag_t.csvImgs': 2, '1_DrDoS_DNS.csvImgs': 3, '2_DrDoS_LDAP.csvImgs': 4, '3_DrDoS_MSSQL.csvImgs': 5, '4_DrDoS_NetBIOS.csvImgs': 6, '5_DrDoS_NTP.csvImgs': 7, '6_DrDoS_SNMP.csvImgs': 8, '7_DrDoS_SSDP.csvImgs': 9, '8_DrDoS_UDP.csvImgs': 10, '9_Normal_12k': 11}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nmodel = models.vgg16(pretrained=True)\nnum_ftrs = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(num_ftrs, 12)\nmodel.to(device)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)\n\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:01:33.002478Z","iopub.execute_input":"2024-10-12T15:01:33.002816Z","iopub.status.idle":"2024-10-12T15:01:37.364426Z","shell.execute_reply.started":"2024-10-12T15:01:33.002783Z","shell.execute_reply":"2024-10-12T15:01:37.363446Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n100%|██████████| 528M/528M [00:02<00:00, 221MB/s]  \n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def train_epoch(dataloader, model, loss_fn, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for i, (inputs, labels) in enumerate(dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        if i % 100 == 99:\n            print(f'Batch {i+1}, Loss: {running_loss/100:.3f}, Accuracy: {100.*correct/total:.2f}%')\n            running_loss = 0.0\n    \n    epoch_accuracy = 100. * correct / total\n    return epoch_accuracy, running_loss / len(dataloader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:01:37.365481Z","iopub.execute_input":"2024-10-12T15:01:37.365786Z","iopub.status.idle":"2024-10-12T15:01:37.374171Z","shell.execute_reply.started":"2024-10-12T15:01:37.365755Z","shell.execute_reply":"2024-10-12T15:01:37.373317Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def evaluate(dataloader, model, loss_fn):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    test_loss = running_loss / len(dataloader)\n    test_accuracy = 100. * correct / total\n    return test_loss, test_accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:01:37.375658Z","iopub.execute_input":"2024-10-12T15:01:37.375938Z","iopub.status.idle":"2024-10-12T15:01:37.392343Z","shell.execute_reply.started":"2024-10-12T15:01:37.375908Z","shell.execute_reply":"2024-10-12T15:01:37.391525Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    train_accuracy, train_loss = train_epoch(train_loader, model, loss_fn, optimizer)\n    test_loss, test_accuracy = evaluate(test_loader, model, loss_fn)\n    \n    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n    print('-' * 40)\n    \n    # Learning rate scheduling\n    scheduler.step(test_loss)\n\nprint('Training completed.')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-12T15:01:37.393626Z","iopub.execute_input":"2024-10-12T15:01:37.393980Z","iopub.status.idle":"2024-10-12T16:43:18.427624Z","shell.execute_reply.started":"2024-10-12T15:01:37.393941Z","shell.execute_reply":"2024-10-12T16:43:18.426570Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/5\nBatch 100, Loss: 1.551, Accuracy: 48.64%\nBatch 200, Loss: 0.511, Accuracy: 64.83%\nBatch 300, Loss: 0.533, Accuracy: 70.63%\nBatch 400, Loss: 0.419, Accuracy: 74.23%\nBatch 500, Loss: 0.436, Accuracy: 76.13%\nBatch 600, Loss: 0.383, Accuracy: 77.95%\nBatch 700, Loss: 0.317, Accuracy: 79.29%\nBatch 800, Loss: 0.302, Accuracy: 80.38%\nBatch 900, Loss: 0.306, Accuracy: 81.21%\nBatch 1000, Loss: 0.313, Accuracy: 81.94%\nBatch 1100, Loss: 0.329, Accuracy: 82.42%\nBatch 1200, Loss: 0.346, Accuracy: 82.82%\nBatch 1300, Loss: 0.281, Accuracy: 83.25%\nBatch 1400, Loss: 0.292, Accuracy: 83.61%\nBatch 1500, Loss: 0.269, Accuracy: 83.92%\nBatch 1600, Loss: 0.271, Accuracy: 84.21%\nBatch 1700, Loss: 0.288, Accuracy: 84.45%\nBatch 1800, Loss: 0.336, Accuracy: 84.63%\nBatch 1900, Loss: 0.257, Accuracy: 84.87%\nBatch 2000, Loss: 0.295, Accuracy: 85.07%\nBatch 2100, Loss: 0.253, Accuracy: 85.25%\nBatch 2200, Loss: 0.278, Accuracy: 85.41%\nBatch 2300, Loss: 0.238, Accuracy: 85.57%\nBatch 2400, Loss: 0.246, Accuracy: 85.72%\nBatch 2500, Loss: 0.250, Accuracy: 85.87%\nBatch 2600, Loss: 0.241, Accuracy: 85.99%\nBatch 2700, Loss: 0.240, Accuracy: 86.13%\nBatch 2800, Loss: 0.280, Accuracy: 86.19%\nBatch 2900, Loss: 0.242, Accuracy: 86.32%\nBatch 3000, Loss: 0.251, Accuracy: 86.42%\nTraining Loss: 0.0030, Training Accuracy: 86.44%\nTest Loss: 1.7720, Test Accuracy: 71.62%\n----------------------------------------\nEpoch 2/5\nBatch 100, Loss: 0.237, Accuracy: 89.47%\nBatch 200, Loss: 0.276, Accuracy: 89.40%\nBatch 300, Loss: 0.290, Accuracy: 89.15%\nBatch 400, Loss: 0.236, Accuracy: 89.34%\nBatch 500, Loss: 0.231, Accuracy: 89.32%\nBatch 600, Loss: 0.240, Accuracy: 89.33%\nBatch 700, Loss: 0.256, Accuracy: 89.33%\nBatch 800, Loss: 0.257, Accuracy: 89.29%\nBatch 900, Loss: 0.234, Accuracy: 89.31%\nBatch 1000, Loss: 0.248, Accuracy: 89.35%\nBatch 1100, Loss: 0.221, Accuracy: 89.39%\nBatch 1200, Loss: 0.219, Accuracy: 89.49%\nBatch 1300, Loss: 0.220, Accuracy: 89.49%\nBatch 1400, Loss: 0.225, Accuracy: 89.51%\nBatch 1500, Loss: 0.256, Accuracy: 89.46%\nBatch 1600, Loss: 0.224, Accuracy: 89.49%\nBatch 1700, Loss: 0.233, Accuracy: 89.51%\nBatch 1800, Loss: 0.270, Accuracy: 89.47%\nBatch 1900, Loss: 0.213, Accuracy: 89.51%\nBatch 2000, Loss: 0.227, Accuracy: 89.53%\nBatch 2100, Loss: 0.202, Accuracy: 89.55%\nBatch 2200, Loss: 0.219, Accuracy: 89.59%\nBatch 2300, Loss: 0.211, Accuracy: 89.61%\nBatch 2400, Loss: 0.206, Accuracy: 89.64%\nBatch 2500, Loss: 0.254, Accuracy: 89.63%\nBatch 2600, Loss: 0.247, Accuracy: 89.62%\nBatch 2700, Loss: 0.271, Accuracy: 89.59%\nBatch 2800, Loss: 0.209, Accuracy: 89.62%\nBatch 2900, Loss: 0.242, Accuracy: 89.63%\nBatch 3000, Loss: 0.221, Accuracy: 89.63%\nTraining Loss: 0.0029, Training Accuracy: 89.62%\nTest Loss: 2.0116, Test Accuracy: 68.14%\n----------------------------------------\nEpoch 3/5\nBatch 100, Loss: 0.204, Accuracy: 89.81%\nBatch 200, Loss: 0.196, Accuracy: 90.20%\nBatch 300, Loss: 0.219, Accuracy: 90.18%\nBatch 400, Loss: 0.199, Accuracy: 90.16%\nBatch 500, Loss: 0.233, Accuracy: 90.22%\nBatch 600, Loss: 0.235, Accuracy: 90.13%\nBatch 700, Loss: 0.292, Accuracy: 89.98%\nBatch 800, Loss: 0.234, Accuracy: 89.96%\nBatch 900, Loss: 0.233, Accuracy: 89.93%\nBatch 1000, Loss: 0.226, Accuracy: 89.89%\nBatch 1100, Loss: 0.215, Accuracy: 89.92%\nBatch 1200, Loss: 0.230, Accuracy: 89.90%\nBatch 1300, Loss: 0.230, Accuracy: 89.86%\nBatch 1400, Loss: 0.228, Accuracy: 89.85%\nBatch 1500, Loss: 0.203, Accuracy: 89.88%\nBatch 1600, Loss: 0.207, Accuracy: 89.94%\nBatch 1700, Loss: 0.216, Accuracy: 89.99%\nBatch 1800, Loss: 0.217, Accuracy: 89.97%\nBatch 1900, Loss: 0.212, Accuracy: 90.00%\nBatch 2000, Loss: 0.223, Accuracy: 89.97%\nBatch 2100, Loss: 0.234, Accuracy: 89.98%\nBatch 2200, Loss: 0.243, Accuracy: 89.96%\nBatch 2300, Loss: 0.233, Accuracy: 89.97%\nBatch 2400, Loss: 0.216, Accuracy: 89.96%\nBatch 2500, Loss: 0.270, Accuracy: 89.94%\nBatch 2600, Loss: 0.260, Accuracy: 89.93%\nBatch 2700, Loss: 0.225, Accuracy: 89.91%\nBatch 2800, Loss: 0.217, Accuracy: 89.92%\nBatch 2900, Loss: 0.209, Accuracy: 89.93%\nBatch 3000, Loss: 0.228, Accuracy: 89.93%\nTraining Loss: 0.0040, Training Accuracy: 89.92%\nTest Loss: 2.6869, Test Accuracy: 68.90%\n----------------------------------------\nEpoch 4/5\nBatch 100, Loss: 0.258, Accuracy: 90.27%\nBatch 200, Loss: 0.222, Accuracy: 89.92%\nBatch 300, Loss: 0.242, Accuracy: 89.74%\nBatch 400, Loss: 0.345, Accuracy: 89.97%\nBatch 500, Loss: 0.225, Accuracy: 89.92%\nBatch 600, Loss: 0.203, Accuracy: 89.92%\nBatch 700, Loss: 0.205, Accuracy: 89.99%\nBatch 800, Loss: 0.228, Accuracy: 89.96%\nBatch 900, Loss: 0.206, Accuracy: 90.01%\nBatch 1000, Loss: 0.243, Accuracy: 90.00%\nBatch 1100, Loss: 0.283, Accuracy: 89.97%\nBatch 1200, Loss: 0.248, Accuracy: 89.95%\nBatch 1300, Loss: 0.214, Accuracy: 89.98%\nBatch 1400, Loss: 0.208, Accuracy: 90.01%\nBatch 1500, Loss: 0.247, Accuracy: 90.01%\nBatch 1600, Loss: 0.214, Accuracy: 90.01%\nBatch 1700, Loss: 0.230, Accuracy: 90.00%\nBatch 1800, Loss: 0.250, Accuracy: 90.02%\nBatch 1900, Loss: 0.210, Accuracy: 90.02%\nBatch 2000, Loss: 0.214, Accuracy: 90.03%\nBatch 2100, Loss: 0.221, Accuracy: 90.03%\nBatch 2200, Loss: 0.197, Accuracy: 90.06%\nBatch 2300, Loss: 0.214, Accuracy: 90.07%\nBatch 2400, Loss: 0.213, Accuracy: 90.05%\nBatch 2500, Loss: 0.201, Accuracy: 90.07%\nBatch 2600, Loss: 0.212, Accuracy: 90.07%\nBatch 2700, Loss: 0.222, Accuracy: 90.05%\nBatch 2800, Loss: 0.220, Accuracy: 90.04%\nBatch 2900, Loss: 0.209, Accuracy: 90.05%\nBatch 3000, Loss: 0.208, Accuracy: 90.06%\nTraining Loss: 0.0027, Training Accuracy: 90.06%\nTest Loss: 4.5193, Test Accuracy: 68.38%\n----------------------------------------\nEpoch 5/5\nBatch 100, Loss: 0.197, Accuracy: 90.36%\nBatch 200, Loss: 0.200, Accuracy: 90.40%\nBatch 300, Loss: 0.187, Accuracy: 90.54%\nBatch 400, Loss: 0.194, Accuracy: 90.43%\nBatch 500, Loss: 0.185, Accuracy: 90.42%\nBatch 600, Loss: 0.197, Accuracy: 90.47%\nBatch 700, Loss: 0.178, Accuracy: 90.53%\nBatch 800, Loss: 0.199, Accuracy: 90.56%\nBatch 900, Loss: 0.191, Accuracy: 90.50%\nBatch 1000, Loss: 0.175, Accuracy: 90.56%\nBatch 1100, Loss: 0.189, Accuracy: 90.58%\nBatch 1200, Loss: 0.180, Accuracy: 90.60%\nBatch 1300, Loss: 0.181, Accuracy: 90.60%\nBatch 1400, Loss: 0.193, Accuracy: 90.55%\nBatch 1500, Loss: 0.192, Accuracy: 90.56%\nBatch 1600, Loss: 0.187, Accuracy: 90.50%\nBatch 1700, Loss: 0.168, Accuracy: 90.57%\nBatch 1800, Loss: 0.192, Accuracy: 90.54%\nBatch 1900, Loss: 0.168, Accuracy: 90.57%\nBatch 2000, Loss: 0.185, Accuracy: 90.56%\nBatch 2100, Loss: 0.167, Accuracy: 90.59%\nBatch 2200, Loss: 0.189, Accuracy: 90.61%\nBatch 2300, Loss: 0.188, Accuracy: 90.62%\nBatch 2400, Loss: 0.193, Accuracy: 90.65%\nBatch 2500, Loss: 0.183, Accuracy: 90.65%\nBatch 2600, Loss: 0.173, Accuracy: 90.68%\nBatch 2700, Loss: 0.188, Accuracy: 90.69%\nBatch 2800, Loss: 0.185, Accuracy: 90.68%\nBatch 2900, Loss: 0.176, Accuracy: 90.67%\nBatch 3000, Loss: 0.181, Accuracy: 90.66%\nTraining Loss: 0.0021, Training Accuracy: 90.67%\nTest Loss: 2.7355, Test Accuracy: 62.47%\n----------------------------------------\nTraining completed.\n","output_type":"stream"}],"execution_count":7}]}