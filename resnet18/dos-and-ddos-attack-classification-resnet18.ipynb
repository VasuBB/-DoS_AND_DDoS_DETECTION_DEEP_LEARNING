{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9608010,"sourceType":"datasetVersion","datasetId":5862350}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport matplotlib.pyplot as plt\n\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:43:46.559309Z","iopub.execute_input":"2024-10-12T13:43:46.559780Z","iopub.status.idle":"2024-10-12T13:43:46.565371Z","shell.execute_reply.started":"2024-10-12T13:43:46.559738Z","shell.execute_reply":"2024-10-12T13:43:46.564429Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Check if CUDA is available and print the status\nif torch.cuda.is_available():\n    print(\"CUDA is available, using GPU\")\n    device = torch.device(\"cuda\")\nelse:\n    print(\"CUDA not available, using CPU\")\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:43:46.566944Z","iopub.execute_input":"2024-10-12T13:43:46.567253Z","iopub.status.idle":"2024-10-12T13:43:46.577742Z","shell.execute_reply.started":"2024-10-12T13:43:46.567220Z","shell.execute_reply":"2024-10-12T13:43:46.576913Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"CUDA is available, using GPU\n","output_type":"stream"}]},{"cell_type":"code","source":"# Transform for data preprocessing\ntransform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Dataset and Dataloaders\ntrain_dataset = datasets.ImageFolder(root='/kaggle/input/ddos-attack/attackImagesPaper/Train', transform=transform)\ntest_dataset = datasets.ImageFolder(root='/kaggle/input/ddos-attack/attackImagesPaper/Test', transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n\nprint(f'Number of classes: {len(train_dataset.classes)}')\nprint(f'Class to index mapping: {train_dataset.class_to_idx}')","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:43:46.583041Z","iopub.execute_input":"2024-10-12T13:43:46.583313Z","iopub.status.idle":"2024-10-12T13:46:13.324098Z","shell.execute_reply.started":"2024-10-12T13:43:46.583282Z","shell.execute_reply":"2024-10-12T13:46:13.323147Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Number of classes: 12\nClass to index mapping: {'10_Syn.csvImgs': 0, '11_TFTP.csvImgs_12k': 1, '12_UDPLag_t.csvImgs': 2, '1_DrDoS_DNS.csvImgs': 3, '2_DrDoS_LDAP.csvImgs': 4, '3_DrDoS_MSSQL.csvImgs': 5, '4_DrDoS_NetBIOS.csvImgs': 6, '5_DrDoS_NTP.csvImgs': 7, '6_DrDoS_SNMP.csvImgs': 8, '7_DrDoS_SSDP.csvImgs': 9, '8_DrDoS_UDP.csvImgs': 10, '9_Normal_12k': 11}\n","output_type":"stream"}]},{"cell_type":"code","source":"\nmodel = models.resnet18(pretrained=True)\nnum_ftrs = model.fc.in_features\nmodel.fc = nn.Linear(num_ftrs, 12)\nmodel.to(device)\n\nloss_fn = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\nscheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, factor=0.5, verbose=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:46:13.326026Z","iopub.execute_input":"2024-10-12T13:46:13.326329Z","iopub.status.idle":"2024-10-12T13:46:13.561565Z","shell.execute_reply.started":"2024-10-12T13:46:13.326295Z","shell.execute_reply":"2024-10-12T13:46:13.560612Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"\n\ndef train_epoch(dataloader, model, loss_fn, optimizer):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for i, (inputs, labels) in enumerate(dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = loss_fn(outputs, labels)\n        loss.backward()\n        \n        # Gradient clipping\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        \n        optimizer.step()\n        \n        running_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += labels.size(0)\n        correct += predicted.eq(labels).sum().item()\n        \n        if i % 100 == 99:\n            print(f'Batch {i+1}, Loss: {running_loss/100:.3f}, Accuracy: {100.*correct/total:.2f}%')\n            running_loss = 0.0\n    \n    epoch_accuracy = 100. * correct / total\n    return epoch_accuracy, running_loss / len(dataloader)\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:46:13.562839Z","iopub.execute_input":"2024-10-12T13:46:13.563143Z","iopub.status.idle":"2024-10-12T13:46:13.571199Z","shell.execute_reply.started":"2024-10-12T13:46:13.563111Z","shell.execute_reply":"2024-10-12T13:46:13.570363Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def evaluate(dataloader, model, loss_fn):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = loss_fn(outputs, labels)\n            \n            running_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += labels.size(0)\n            correct += predicted.eq(labels).sum().item()\n    \n    test_loss = running_loss / len(dataloader)\n    test_accuracy = 100. * correct / total\n    return test_loss, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:46:13.572292Z","iopub.execute_input":"2024-10-12T13:46:13.572610Z","iopub.status.idle":"2024-10-12T13:46:13.588709Z","shell.execute_reply.started":"2024-10-12T13:46:13.572554Z","shell.execute_reply":"2024-10-12T13:46:13.587818Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"num_epochs = 5\nfor epoch in range(num_epochs):\n    print(f'Epoch {epoch+1}/{num_epochs}')\n    train_accuracy, train_loss = train_epoch(train_loader, model, loss_fn, optimizer)\n    test_loss, test_accuracy = evaluate(test_loader, model, loss_fn)\n    \n    print(f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%')\n    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%')\n    print('-' * 40)\n    \n    # Learning rate scheduling\n    scheduler.step(test_loss)\n\nprint('Training completed.')","metadata":{"execution":{"iopub.status.busy":"2024-10-12T13:46:13.590623Z","iopub.execute_input":"2024-10-12T13:46:13.590930Z","iopub.status.idle":"2024-10-12T14:48:23.676940Z","shell.execute_reply.started":"2024-10-12T13:46:13.590898Z","shell.execute_reply":"2024-10-12T14:48:23.675995Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/5\nBatch 100, Loss: 0.440, Accuracy: 83.00%\nBatch 200, Loss: 0.303, Accuracy: 85.17%\nBatch 300, Loss: 0.267, Accuracy: 86.29%\nBatch 400, Loss: 0.242, Accuracy: 86.90%\nBatch 500, Loss: 0.231, Accuracy: 87.30%\nBatch 600, Loss: 0.228, Accuracy: 87.75%\nBatch 700, Loss: 0.224, Accuracy: 88.13%\nBatch 800, Loss: 0.204, Accuracy: 88.45%\nBatch 900, Loss: 0.212, Accuracy: 88.66%\nBatch 1000, Loss: 0.199, Accuracy: 88.86%\nBatch 1100, Loss: 0.192, Accuracy: 89.09%\nBatch 1200, Loss: 0.209, Accuracy: 89.18%\nBatch 1300, Loss: 0.194, Accuracy: 89.31%\nBatch 1400, Loss: 0.210, Accuracy: 89.30%\nBatch 1500, Loss: 0.200, Accuracy: 89.35%\nBatch 1600, Loss: 0.192, Accuracy: 89.44%\nBatch 1700, Loss: 0.185, Accuracy: 89.55%\nBatch 1800, Loss: 0.213, Accuracy: 89.58%\nBatch 1900, Loss: 0.196, Accuracy: 89.65%\nBatch 2000, Loss: 0.202, Accuracy: 89.67%\nBatch 2100, Loss: 0.181, Accuracy: 89.75%\nBatch 2200, Loss: 0.185, Accuracy: 89.82%\nBatch 2300, Loss: 0.201, Accuracy: 89.81%\nBatch 2400, Loss: 0.195, Accuracy: 89.83%\nBatch 2500, Loss: 0.176, Accuracy: 89.91%\nBatch 2600, Loss: 0.184, Accuracy: 89.95%\nBatch 2700, Loss: 0.184, Accuracy: 90.00%\nBatch 2800, Loss: 0.179, Accuracy: 90.04%\nBatch 2900, Loss: 0.181, Accuracy: 90.07%\nBatch 3000, Loss: 0.179, Accuracy: 90.11%\nTraining Loss: 0.0021, Training Accuracy: 90.12%\nTest Loss: 1.3134, Test Accuracy: 76.97%\n----------------------------------------\nEpoch 2/5\nBatch 100, Loss: 0.195, Accuracy: 90.89%\nBatch 200, Loss: 0.184, Accuracy: 90.91%\nBatch 300, Loss: 0.178, Accuracy: 91.17%\nBatch 400, Loss: 0.181, Accuracy: 91.21%\nBatch 500, Loss: 0.186, Accuracy: 91.21%\nBatch 600, Loss: 0.183, Accuracy: 91.21%\nBatch 700, Loss: 0.181, Accuracy: 91.15%\nBatch 800, Loss: 0.177, Accuracy: 91.17%\nBatch 900, Loss: 0.177, Accuracy: 91.18%\nBatch 1000, Loss: 0.172, Accuracy: 91.18%\nBatch 1100, Loss: 0.171, Accuracy: 91.18%\nBatch 1200, Loss: 0.179, Accuracy: 91.23%\nBatch 1300, Loss: 0.175, Accuracy: 91.25%\nBatch 1400, Loss: 0.158, Accuracy: 91.33%\nBatch 1500, Loss: 0.190, Accuracy: 91.31%\nBatch 1600, Loss: 0.173, Accuracy: 91.31%\nBatch 1700, Loss: 0.170, Accuracy: 91.34%\nBatch 1800, Loss: 0.166, Accuracy: 91.37%\nBatch 1900, Loss: 0.171, Accuracy: 91.39%\nBatch 2000, Loss: 0.162, Accuracy: 91.41%\nBatch 2100, Loss: 0.169, Accuracy: 91.44%\nBatch 2200, Loss: 0.171, Accuracy: 91.44%\nBatch 2300, Loss: 0.176, Accuracy: 91.46%\nBatch 2400, Loss: 0.180, Accuracy: 91.48%\nBatch 2500, Loss: 0.180, Accuracy: 91.50%\nBatch 2600, Loss: 0.176, Accuracy: 91.50%\nBatch 2700, Loss: 0.175, Accuracy: 91.50%\nBatch 2800, Loss: 0.178, Accuracy: 91.50%\nBatch 2900, Loss: 0.183, Accuracy: 91.50%\nBatch 3000, Loss: 0.175, Accuracy: 91.51%\nTraining Loss: 0.0020, Training Accuracy: 91.51%\nTest Loss: 3.0416, Test Accuracy: 57.77%\n----------------------------------------\nEpoch 3/5\nBatch 100, Loss: 0.178, Accuracy: 91.34%\nBatch 200, Loss: 0.181, Accuracy: 91.48%\nBatch 300, Loss: 0.167, Accuracy: 91.79%\nBatch 400, Loss: 0.163, Accuracy: 91.91%\nBatch 500, Loss: 0.175, Accuracy: 91.91%\nBatch 600, Loss: 0.162, Accuracy: 91.97%\nBatch 700, Loss: 0.164, Accuracy: 91.99%\nBatch 800, Loss: 0.162, Accuracy: 92.01%\nBatch 900, Loss: 0.161, Accuracy: 92.04%\nBatch 1000, Loss: 0.160, Accuracy: 92.07%\nBatch 1100, Loss: 0.168, Accuracy: 92.04%\nBatch 1200, Loss: 0.161, Accuracy: 92.07%\nBatch 1300, Loss: 0.174, Accuracy: 92.02%\nBatch 1400, Loss: 0.177, Accuracy: 91.99%\nBatch 1500, Loss: 0.175, Accuracy: 91.94%\nBatch 1600, Loss: 0.167, Accuracy: 91.95%\nBatch 1700, Loss: 0.172, Accuracy: 91.93%\nBatch 1800, Loss: 0.164, Accuracy: 91.91%\nBatch 1900, Loss: 0.168, Accuracy: 91.92%\nBatch 2000, Loss: 0.156, Accuracy: 91.92%\nBatch 2100, Loss: 0.168, Accuracy: 91.93%\nBatch 2200, Loss: 0.169, Accuracy: 91.95%\nBatch 2300, Loss: 0.158, Accuracy: 91.97%\nBatch 2400, Loss: 0.168, Accuracy: 91.97%\nBatch 2500, Loss: 0.162, Accuracy: 91.98%\nBatch 2600, Loss: 0.160, Accuracy: 91.98%\nBatch 2700, Loss: 0.175, Accuracy: 91.98%\nBatch 2800, Loss: 0.171, Accuracy: 91.98%\nBatch 2900, Loss: 0.186, Accuracy: 91.98%\nBatch 3000, Loss: 0.176, Accuracy: 91.98%\nTraining Loss: 0.0020, Training Accuracy: 91.98%\nTest Loss: 1.3354, Test Accuracy: 74.47%\n----------------------------------------\nEpoch 4/5\nBatch 100, Loss: 0.165, Accuracy: 92.23%\nBatch 200, Loss: 0.175, Accuracy: 91.97%\nBatch 300, Loss: 0.162, Accuracy: 91.97%\nBatch 400, Loss: 0.166, Accuracy: 91.96%\nBatch 500, Loss: 0.175, Accuracy: 91.99%\nBatch 600, Loss: 0.161, Accuracy: 91.99%\nBatch 700, Loss: 0.167, Accuracy: 92.00%\nBatch 800, Loss: 0.162, Accuracy: 92.04%\nBatch 900, Loss: 0.158, Accuracy: 92.09%\nBatch 1000, Loss: 0.162, Accuracy: 92.13%\nBatch 1100, Loss: 0.166, Accuracy: 92.12%\nBatch 1200, Loss: 0.153, Accuracy: 92.15%\nBatch 1300, Loss: 0.165, Accuracy: 92.17%\nBatch 1400, Loss: 0.158, Accuracy: 92.20%\nBatch 1500, Loss: 0.162, Accuracy: 92.19%\nBatch 1600, Loss: 0.164, Accuracy: 92.20%\nBatch 1700, Loss: 0.155, Accuracy: 92.20%\nBatch 1800, Loss: 0.164, Accuracy: 92.21%\nBatch 1900, Loss: 0.153, Accuracy: 92.23%\nBatch 2000, Loss: 0.167, Accuracy: 92.22%\nBatch 2100, Loss: 0.172, Accuracy: 92.19%\nBatch 2200, Loss: 0.171, Accuracy: 92.18%\nBatch 2300, Loss: 0.161, Accuracy: 92.19%\nBatch 2400, Loss: 0.166, Accuracy: 92.20%\nBatch 2500, Loss: 0.165, Accuracy: 92.19%\nBatch 2600, Loss: 0.161, Accuracy: 92.19%\nBatch 2700, Loss: 0.167, Accuracy: 92.19%\nBatch 2800, Loss: 0.167, Accuracy: 92.19%\nBatch 2900, Loss: 0.158, Accuracy: 92.20%\nBatch 3000, Loss: 0.162, Accuracy: 92.20%\nTraining Loss: 0.0022, Training Accuracy: 92.19%\nTest Loss: 1.8417, Test Accuracy: 74.53%\n----------------------------------------\nEpoch 5/5\nBatch 100, Loss: 0.146, Accuracy: 92.91%\nBatch 200, Loss: 0.152, Accuracy: 92.80%\nBatch 300, Loss: 0.149, Accuracy: 92.83%\nBatch 400, Loss: 0.148, Accuracy: 92.86%\nBatch 500, Loss: 0.153, Accuracy: 92.72%\nBatch 600, Loss: 0.145, Accuracy: 92.76%\nBatch 700, Loss: 0.146, Accuracy: 92.76%\nBatch 800, Loss: 0.158, Accuracy: 92.72%\nBatch 900, Loss: 0.151, Accuracy: 92.74%\nBatch 1000, Loss: 0.159, Accuracy: 92.71%\nBatch 1100, Loss: 0.155, Accuracy: 92.70%\nBatch 1200, Loss: 0.158, Accuracy: 92.67%\nBatch 1300, Loss: 0.160, Accuracy: 92.64%\nBatch 1400, Loss: 0.161, Accuracy: 92.60%\nBatch 1500, Loss: 0.150, Accuracy: 92.62%\nBatch 1600, Loss: 0.154, Accuracy: 92.63%\nBatch 1700, Loss: 0.146, Accuracy: 92.63%\nBatch 1800, Loss: 0.153, Accuracy: 92.62%\nBatch 1900, Loss: 0.147, Accuracy: 92.63%\nBatch 2000, Loss: 0.158, Accuracy: 92.63%\nBatch 2100, Loss: 0.155, Accuracy: 92.63%\nBatch 2200, Loss: 0.151, Accuracy: 92.64%\nBatch 2300, Loss: 0.149, Accuracy: 92.64%\nBatch 2400, Loss: 0.155, Accuracy: 92.65%\nBatch 2500, Loss: 0.148, Accuracy: 92.66%\nBatch 2600, Loss: 0.155, Accuracy: 92.66%\nBatch 2700, Loss: 0.154, Accuracy: 92.67%\nBatch 2800, Loss: 0.152, Accuracy: 92.68%\nBatch 2900, Loss: 0.158, Accuracy: 92.67%\nBatch 3000, Loss: 0.157, Accuracy: 92.66%\nTraining Loss: 0.0018, Training Accuracy: 92.67%\nTest Loss: 1.5151, Test Accuracy: 74.84%\n----------------------------------------\nTraining completed.\n","output_type":"stream"}]}]}